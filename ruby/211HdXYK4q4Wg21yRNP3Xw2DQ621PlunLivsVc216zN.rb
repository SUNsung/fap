
        
                MergeRequest
          .where(id: start_id..stop_id)
          .where(latest_merge_request_diff_id: nil)
          .each_batch(of: BATCH_SIZE) do |relation|
    
            def preload_commit_authors
          # This also preloads the author of every commit. We're using 'lazy_author'
          # here since 'author' immediately loads the data on the first call.
          @pipeline.commit.try(:lazy_author)
        end
    
            Redis::Cache.with do |redis|
          redis.set(key, value, ex: timeout)
        end
    
            def sidekiq_worker_class
          ImportIssueWorker
        end
    
            def sidekiq_worker_class
          ImportLfsObjectWorker
        end
    
            # We inject the page number here to make sure that all importers always
        # start where they left off. Simply starting over wouldn't work for
        # repositories with a lot of data (e.g. tens of thousands of comments).
        options = collection_options.merge(page: page_counter.current)
    
      private
    
            PluginManager.ui.debug('Looking if package named: #{plugin_name} exists at #{uri}')
    
              it 'fails when installing a non logstash plugin' do
            command = logstash.run_command_in_path('bin/logstash-plugin install  bundler')
            expect(command).not_to install_successfully
          end